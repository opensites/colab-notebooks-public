{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opensites/colab-notebooks-public/blob/main/models/GPT_J_sample_sight_word_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install enum"
      ],
      "metadata": {
        "id": "SGUdKHLob1hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Tensorflow 2.0\n",
        "import enum\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "\n",
        "# Import all remaining packages\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Check that we are using a GPU, if not switch runtimes\n",
        "#   using Runtime > Change Runtime Type > GPU\n",
        "assert len(tf.config.list_physical_devices('GPU')) >= 0"
      ],
      "metadata": {
        "id": "WIjzWfBzc5bj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can upload the data files from our computer (access using the [tutorial linked here](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92))"
      ],
      "metadata": {
        "id": "uT8STxl9ekw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "D5yC65RRdicS",
        "outputId": "ef7a79bb-45e7-4b3b-b275-f01e87e34137"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f1c5151c-905f-4f6e-88c8-2e41f9345ab5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f1c5151c-905f-4f6e-88c8-2e41f9345ab5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train-prompt2.csv to train-prompt2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "import io\n",
        "df_prompt2 = pd.read_csv(io.BytesIO(uploaded['train-prompt2.csv']))\n",
        "df_prompt2[\"Response\"] = \"startseqtok \" + df_prompt2[\"Response\"] + \" endseqtok\"\n",
        "print(df_prompt2[\"Response\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMF-wmQBeaBh",
        "outputId": "e2f5477f-3913-45d8-97cc-c00f33e8d399"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    startseqtok The frog is by the cat on the log....\n",
            "1    startseqtok The cat may play with the rat day....\n",
            "2         startseqtok The mat is by the cat. endseqtok\n",
            "3                 startseqtok The sun is up. endseqtok\n",
            "4             startseqtok It is sunny today. endseqtok\n",
            "Name: Response, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "all_data_tensor = tf.constant(df_prompt2.astype(str).values)\n",
        "condensed_data_tensor = tf.reshape(all_data_tensor,[-1])\n",
        "\n",
        "# number of unique words in the dataset after punctuation filtering\n",
        "word_count_layer = layers.TextVectorization()\n",
        "word_count_layer.adapt(condensed_data_tensor)\n",
        "num_words = len(word_count_layer.get_vocabulary())\n",
        "print(f'There are {num_words} unique words in this dataset')\n",
        "print(word_count_layer.get_vocabulary()[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzDu7hW9fjyF",
        "outputId": "1c8953be-6313-4598-dc15-2107d2f5e966"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 753 unique words in this dataset\n",
            "['', '[UNK]', 'the', 'startseqtok', 'endseqtok', 'is', 'i', 'a', 'to', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer\n",
        "# https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
        "# https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "max_features = math.floor(num_words * 1.5)\n",
        "sequence_length = 25\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    ngrams=3,\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# build vocab for this dataset\n",
        "vectorize_layer.adapt(condensed_data_tensor)\n",
        "\n",
        "# save the vocabulary as a standard python list\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "\n",
        "# how far did ngram truly go\n",
        "max_ngram_size = 0\n",
        "for item in vocab:\n",
        "    max_ngram_size = max(max_ngram_size, len(item.split()))\n",
        "\n",
        "\n",
        "print(f'The maximum ngram found was {max_ngram_size}.')\n",
        "\n",
        "# # see some vocab details\n",
        "print(\" 10 ---> \", vocab[10])\n",
        "print(\" 50 ---> \", vocab[50])\n",
        "print(\" -1 ---> \", vocab[-1])\n",
        "print(f'Vocabulary size: {len(vocab)}')\n",
        "print('startseqtok' in vocab)\n",
        "print('endseqtok' in vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pu3wrknmzjd",
        "outputId": "2fa7fc62-315c-4773-b168-285a550e6198"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum ngram found was 3.\n",
            " 10 --->  cat\n",
            " 50 --->  the sun\n",
            " -1 --->  startseqtok i said\n",
            "Vocabulary size: 1129\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_dataset_partitions_pd(df, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "\n",
        "    total_split = train_split + test_split + val_split\n",
        "    if total_split != 1:\n",
        "      train_split = train_split / total_split\n",
        "      test_split = test_split / total_split\n",
        "      val_split = val_split / total_split\n",
        "\n",
        "    # Specify seed to always have the same split distribution between runs\n",
        "    df_shuffled = df.sample(frac=1, random_state=12)\n",
        "    indices_or_sections = [int(train_split * len(df)), int((train_split + val_split) * len(df))]\n",
        "    \n",
        "    train_ds, val_ds, test_ds = np.split(df_shuffled, indices_or_sections)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "-KwPYKLtVkbz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see article here: https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\n",
        "# and docs here: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
        "    \n",
        "    ds_size = ds.cardinality().numpy()\n",
        "    total_split = train_split + test_split + val_split\n",
        "    if total_split != 1:\n",
        "      train_split = train_split / total_split\n",
        "      test_split = test_split / total_split\n",
        "      val_split = val_split / total_split\n",
        "\n",
        "    if shuffle:\n",
        "        # Specify seed to always have the same split distribution between runs\n",
        "        ds = ds.shuffle(shuffle_size, seed=2011)\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "Zjkr6_w9JlDT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_all = tf.data.Dataset.from_tensor_slices(all_data_tensor)\n",
        "train_ds, val_ds, test_ds = get_dataset_partitions_tf(ds_all)"
      ],
      "metadata": {
        "id": "RpIAkrCpLp35"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_pd, val_ds_pd, test_ds_pd = get_dataset_partitions_pd(df_prompt2)\n",
        "train_ds_tensor = tf.constant(train_ds_pd.astype(str).values)\n",
        "val_ds_tensor = tf.constant(val_ds_pd.astype(str).values)\n",
        "test_ds_tensor = tf.constant(test_ds_pd.astype(str).values)\n",
        "\n",
        "vector_prompts = vectorize_layer(train_ds_tensor[:,0])\n",
        "vector_responses = vectorize_layer(train_ds_tensor[:,1])\n",
        "\n",
        "print(train_ds_tensor[4,1])\n",
        "print(vector_prompts[0])\n",
        "print(vector_responses[4])\n",
        "vocab[3]"
      ],
      "metadata": {
        "id": "3MTBzcRlXyTl",
        "outputId": "f23a92e5-1504-4fc2-fe9e-04e0a1fbcfa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\"startseqtok Who doesn't? endseqtok\", shape=(), dtype=string)\n",
            "tf.Tensor(\n",
            "[ 13   9  11   2  10 170 172  22  15 204 245 174   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0], shape=(25,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[  3 195 242   4 217 230 241 234 229   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0], shape=(25,), dtype=int64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'startseqtok'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize the input\n",
        "vector_prompts = vectorize_layer(train_ds)"
      ],
      "metadata": {
        "id": "5iuA1NppVOBe",
        "outputId": "f1171dee-ef88-4861-b65e-ccec35ca3c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-30b286953929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# vectorize the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvector_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"text_vectorization_3\" (type TextVectorization).\n\nAttempt to convert a value (<TakeDataset element_spec=TensorSpec(shape=(2,), dtype=tf.string, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>) to a Tensor.\n\nCall arguments received by layer \"text_vectorization_3\" (type TextVectorization):\n  • inputs=<TakeDataset element_spec=TensorSpec(shape=(2,), dtype=tf.string, name=None)>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Z-SjWw7p0Ph3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "08a1f489-a762-40d8-db6e-7c4795ea8e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum ngram found was 3.\n",
            " 10 --->  cat\n",
            " 50 --->  the sun\n",
            " -1 --->  yes i\n",
            "Vocabulary size: 188\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-efc84215acd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# vectorize the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m vector_target = (df_prompt2.loc[:, ['recycle', 'review', 'action']].astype(\n\u001b[0m\u001b[1;32m     43\u001b[0m     str) == 'x').astype(float).values\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    804\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m             \u001b[0;31m# We should never have retval.ndim < self.ndim, as that should\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0;31m#  be handled by the _getitem_lowerdim call above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1151\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[1;32m   1095\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['recycle', 'review', 'action'], dtype='object')] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "# def vectorize_text(text, label):\n",
        "#   text = tf.expand_dims(text, -1)\n",
        "#   return vectorize_layer(text), label\n",
        "\n",
        "# vectorize the input\n",
        "vector_prompts = vectorize_layer(condensed_data_tensor)\n",
        "\n",
        "# vectorize the output\n",
        "vector_target = (df_prompt2.loc[:, ['recycle', 'review', 'action']].astype(\n",
        "    str) == 'x').astype(float).values\n",
        "\n",
        "### Batch definition to create training examples ###\n",
        "\n",
        "\n",
        "def get_batch(vectorized_chats, vectorized_target, batch_size):\n",
        "\n",
        "    # number of chats\n",
        "    n = vectorized_chats.shape[0]\n",
        "\n",
        "    # randomly choose the starting indices for the examples in the training batch\n",
        "    sample_indices = np.random.choice(n, batch_size)\n",
        "\n",
        "    # x_batch, y_batch provide the true inputs and targets for network training\n",
        "    x_batch = tf.constant(vectorized_chats[sample_indices, :])\n",
        "    y_batch = tf.constant(vectorized_target[sample_indices, :])\n",
        "\n",
        "    return x_batch, y_batch\n",
        "\n",
        "# define the LSTM\n",
        "\n",
        "\n",
        "def LSTM(rnn_units, stateful=True):\n",
        "    return tf.keras.layers.LSTM(\n",
        "        rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        recurrent_activation='sigmoid',\n",
        "        stateful=stateful,\n",
        "    )\n",
        "\n",
        "### Defining the RNN Model ###\n",
        "\n",
        "\n",
        "def build_model(vocab_size, num_class, embedding_dim, rnn_units, batch_size):\n",
        "\n",
        "    first_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True) if batch_size is None else tf.keras.layers.Embedding(\n",
        "        vocab_size, embedding_dim, batch_input_shape=[batch_size, None], mask_zero=True)\n",
        "\n",
        "    LSTM_layer = LSTM(rnn_units, stateful=False) if batch_size is None else LSTM(rnn_units)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "\n",
        "        # Layer 0: mask zeros in time steps, i.e., data does not exist\n",
        "        # https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking\n",
        "        # https://www.tensorflow.org/guide/keras/masking_and_padding\n",
        "        # tf.keras.layers.Masking(mask_value=0.0),\n",
        "\n",
        "        # Layer 1: Embedding layer to transform indices into dense vectors\n",
        "        #   of a fixed embedding size\n",
        "        # mask zeros for diff length inputs\n",
        "        first_layer,\n",
        "\n",
        "        # dropout to prevent overfitting\n",
        "        tf.keras.layers.Dropout(.2),\n",
        "\n",
        "        # Layer 2: LSTM with `rnn_units` number of units.\n",
        "        LSTM_layer,\n",
        "\n",
        "        # dropout to prevent overfitting\n",
        "        tf.keras.layers.Dropout(.2),\n",
        "\n",
        "        # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\n",
        "        #   into the vocabulary size. NOTE: output will need to have softmax applied...no activation\n",
        "        tf.keras.layers.Dense(num_class)\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# loss function, negative log likelihood\n",
        "\n",
        "\n",
        "def compute_loss(labels, logits):\n",
        "    # from_logits means we compare against the output probability distribution\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "        labels, logits, from_logits=True)\n",
        "    return loss\n",
        "\n",
        "\n",
        "#################################################################\n",
        "### Hyperparameter setting and optimization ###\n",
        "# Optimization parameters:\n",
        "num_training_iterations = 20001  # Increase this to train longer\n",
        "batch_size = 32  # Experiment between 1 and 64\n",
        "initial_learning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n",
        "\n",
        "# Model parameters:\n",
        "num_classes = 3\n",
        "embedding_dim = 8\n",
        "rnn_units = 128  # Experiment between 1 and 2048\n",
        "\n",
        "# Checkpoint location:\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")\n",
        "\n",
        "### Define optimizer and training operation ###\n",
        "# Build a simple model with above hyperparameters.\n",
        "model = build_model(vocab_size=len(vocab), num_class=num_classes,\n",
        "                    embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
        "\n",
        "# learning rate schedule\n",
        "# see here:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=7500,\n",
        "    decay_rate=0.1,\n",
        "    staircase=True)\n",
        "\n",
        "\n",
        "# specific gradient descent algorithm choice.\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=lr_schedule,\n",
        ")\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    # Use tf.GradientTape()\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # generate predictions. Use training=True since we have dropout (acts differently in tng vs inference,\n",
        "        # see here: https://www.tensorflow.org/tutorials/quickstart/advanced)\n",
        "        y_hat = model(x, training=True)\n",
        "\n",
        "        # get the input label\n",
        "        y_sample = tf.math.argmax(y, axis=1, output_type=tf.dtypes.int64)\n",
        "        # tf.print(y, summarize=-1)\n",
        "        # tf.print(y_sample, summarize=-1)\n",
        "\n",
        "        loss = compute_loss(y_sample, y_hat[:, -1, :])\n",
        "\n",
        "        # Compute the gradients\n",
        "        # We want the gradient of the loss with respect to all of the model parameters.\n",
        "        # Use `model.trainable_variables` to get a list of all model parameters.\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Apply the gradients to the optimizer so it can update the model accordingly\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "##################\n",
        "# Begin training!#\n",
        "##################\n",
        "\n",
        "# history = []\n",
        "# # plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
        "# plt.title('Loss over time')\n",
        "# plt.xlabel('Iterations')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "# if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "# for iter in tqdm(range(num_training_iterations)):\n",
        "\n",
        "#   # Grab a batch and propagate it through the network\n",
        "#   x_batch, y_batch = get_batch(vector_chats.numpy(), vector_target, batch_size=batch_size)\n",
        "#   loss = train_step(x_batch, y_batch)\n",
        "\n",
        "# #   print(\"Input shape:      \", x_batch.shape, \" # (batch_size, sequence_length)\")\n",
        "# #   print(\"Prediction shape: \", y_batch.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "#   # Update the progress bar\n",
        "#   history.append(loss.numpy().mean())\n",
        "\n",
        "#   # Update the model with the changed weights!\n",
        "#   if iter % 100 == 0:\n",
        "#     model.save_weights(checkpoint_prefix)\n",
        "#     plt.plot(range(iter + 1), history, 'g', label='Training loss')\n",
        "#     plt.show()\n",
        "\n",
        "# # Save the trained model and the weights\n",
        "# model.save_weights(checkpoint_prefix)\n",
        "# # plt.plot(epochs, loss_val, 'b', label='validation loss')    plt.show()\n",
        "# plt.plot(range(101,num_training_iterations), history[101:num_training_iterations], 'g', label='Training loss')\n",
        "\n",
        "\n",
        "############## Inference ######################\n",
        "\n",
        "\n",
        "# batch size None for inference, remove statefulness and allow any size input\n",
        "model = build_model(vocab_size=len(vocab), num_class=num_classes, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=None)\n",
        "\n",
        "# Restore the model weights for the last checkpoint after training\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "## Prediction of a chat class ###\n",
        "def classify(chats):\n",
        "\n",
        "    # convert to list if needed\n",
        "    chats = [chats] if isinstance(chats, str) else chats\n",
        "\n",
        "    # up the dimension if single inference...its hacky i know...\n",
        "    single_item = False\n",
        "    if len(chats) == 1:\n",
        "       chats.append(\"\")\n",
        "       single_item = True\n",
        "\n",
        "    # Evaluation step (generating ABC text using the learned RNN model)\n",
        "    input_eval = vectorize_layer(tf.squeeze(chats))\n",
        "    pred = model(input_eval)\n",
        "    pred = tf.nn.softmax(tf.squeeze(pred)[:, -1, :])\n",
        "    output_labels = tf.argmax(pred, axis=1)\n",
        "\n",
        "    return output_labels if not single_item else [output_labels[0]]\n",
        "\n",
        "\n",
        "def classify_label(chats):\n",
        "    encoded_labels = classify(chats)\n",
        "    labels = ['recycle', 'review', 'action']\n",
        "    return list(map(lambda label: labels[label], encoded_labels))\n",
        "\n",
        "# calculate accuracy\n",
        "pred_labels = classify(condensed_data_tensor)\n",
        "accuracy = np.sum(np.equal(\n",
        "    list(map(np.argmax, vector_target)), pred_labels))/len(condensed_data_tensor)\n",
        "print(f'Accuracy is {accuracy}')"
      ]
    }
  ]
}