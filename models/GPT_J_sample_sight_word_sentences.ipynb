{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opensites/colab-notebooks-public/blob/main/models/GPT_J_sample_sight_word_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install enum"
      ],
      "metadata": {
        "id": "SGUdKHLob1hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Tensorflow 2.0\n",
        "import enum\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "\n",
        "# Import all remaining packages\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Check that we are using a GPU, if not switch runtimes\n",
        "#   using Runtime > Change Runtime Type > GPU\n",
        "assert len(tf.config.list_physical_devices('GPU')) >= 0"
      ],
      "metadata": {
        "id": "WIjzWfBzc5bj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "D5yC65RRdicS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-SjWw7p0Ph3"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "chat816 = pd.read_csv(\n",
        "    '../../../data/(EROB) MM_Dataset_816_CSVsanitized_flights.csv')\n",
        "\n",
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = tf.constant(chat816['Column12'].astype(str).values)\n",
        "\n",
        "# number of unique words in the dataset after punctuation filtering\n",
        "word_count_layer = layers.TextVectorization()\n",
        "word_count_layer.adapt(train_text)\n",
        "num_words = len(word_count_layer.get_vocabulary())\n",
        "print(f'There are {num_words} unique words in this dataset')\n",
        "\n",
        "# tokenizer\n",
        "# https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
        "# https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "max_features = math.floor(num_words * .25)\n",
        "sequence_length = 25\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    ngrams=3,\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# build vocab for this dataset\n",
        "vectorize_layer.adapt(train_text)\n",
        "\n",
        "# save the vocabulary as a standard python list\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "\n",
        "# how far did ngram truly go\n",
        "max_ngram_size = 0\n",
        "for item in vocab:\n",
        "    max_ngram_size = max(max_ngram_size, len(item.split()))\n",
        "\n",
        "\n",
        "print(f'The maximum ngram found was {max_ngram_size}.')\n",
        "\n",
        "# # see some vocab details\n",
        "print(\" 10 ---> \", vocab[10])\n",
        "print(\" 50 ---> \", vocab[50])\n",
        "print(\" -1 ---> \", vocab[-1])\n",
        "print(f'Vocabulary size: {len(vocab)}')\n",
        "\n",
        "# def vectorize_text(text, label):\n",
        "#   text = tf.expand_dims(text, -1)\n",
        "#   return vectorize_layer(text), label\n",
        "\n",
        "# vectorize the input\n",
        "vector_chats = vectorize_layer(train_text)\n",
        "\n",
        "# vectorize the output\n",
        "vector_target = (chat816.loc[:, ['recycle', 'review', 'action']].astype(\n",
        "    str) == 'x').astype(float).values\n",
        "\n",
        "### Batch definition to create training examples ###\n",
        "\n",
        "\n",
        "def get_batch(vectorized_chats, vectorized_target, batch_size):\n",
        "\n",
        "    # number of chats\n",
        "    n = vectorized_chats.shape[0]\n",
        "\n",
        "    # randomly choose the starting indices for the examples in the training batch\n",
        "    sample_indices = np.random.choice(n, batch_size)\n",
        "\n",
        "    # x_batch, y_batch provide the true inputs and targets for network training\n",
        "    x_batch = tf.constant(vectorized_chats[sample_indices, :])\n",
        "    y_batch = tf.constant(vectorized_target[sample_indices, :])\n",
        "\n",
        "    return x_batch, y_batch\n",
        "\n",
        "# define the LSTM\n",
        "\n",
        "\n",
        "def LSTM(rnn_units, stateful=True):\n",
        "    return tf.keras.layers.LSTM(\n",
        "        rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        recurrent_activation='sigmoid',\n",
        "        stateful=stateful,\n",
        "    )\n",
        "\n",
        "### Defining the RNN Model ###\n",
        "\n",
        "\n",
        "def build_model(vocab_size, num_class, embedding_dim, rnn_units, batch_size):\n",
        "\n",
        "    first_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True) if batch_size is None else tf.keras.layers.Embedding(\n",
        "        vocab_size, embedding_dim, batch_input_shape=[batch_size, None], mask_zero=True)\n",
        "\n",
        "    LSTM_layer = LSTM(rnn_units, stateful=False) if batch_size is None else LSTM(rnn_units)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "\n",
        "        # Layer 0: mask zeros in time steps, i.e., data does not exist\n",
        "        # https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking\n",
        "        # https://www.tensorflow.org/guide/keras/masking_and_padding\n",
        "        # tf.keras.layers.Masking(mask_value=0.0),\n",
        "\n",
        "        # Layer 1: Embedding layer to transform indices into dense vectors\n",
        "        #   of a fixed embedding size\n",
        "        # mask zeros for diff length inputs\n",
        "        first_layer,\n",
        "\n",
        "        # dropout to prevent overfitting\n",
        "        tf.keras.layers.Dropout(.2),\n",
        "\n",
        "        # Layer 2: LSTM with `rnn_units` number of units.\n",
        "        LSTM_layer,\n",
        "\n",
        "        # dropout to prevent overfitting\n",
        "        tf.keras.layers.Dropout(.2),\n",
        "\n",
        "        # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\n",
        "        #   into the vocabulary size. NOTE: output will need to have softmax applied...no activation\n",
        "        tf.keras.layers.Dense(num_class)\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# loss function, negative log likelihood\n",
        "\n",
        "\n",
        "def compute_loss(labels, logits):\n",
        "    # from_logits means we compare against the output probability distribution\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "        labels, logits, from_logits=True)\n",
        "    return loss\n",
        "\n",
        "\n",
        "#################################################################\n",
        "### Hyperparameter setting and optimization ###\n",
        "# Optimization parameters:\n",
        "num_training_iterations = 20001  # Increase this to train longer\n",
        "batch_size = 32  # Experiment between 1 and 64\n",
        "initial_learning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n",
        "\n",
        "# Model parameters:\n",
        "num_classes = 3\n",
        "embedding_dim = 8\n",
        "rnn_units = 128  # Experiment between 1 and 2048\n",
        "\n",
        "# Checkpoint location:\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")\n",
        "\n",
        "### Define optimizer and training operation ###\n",
        "# Build a simple model with above hyperparameters.\n",
        "model = build_model(vocab_size=len(vocab), num_class=num_classes,\n",
        "                    embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
        "\n",
        "# learning rate schedule\n",
        "# see here:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=7500,\n",
        "    decay_rate=0.1,\n",
        "    staircase=True)\n",
        "\n",
        "\n",
        "# specific gradient descent algorithm choice.\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=lr_schedule,\n",
        ")\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    # Use tf.GradientTape()\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # generate predictions. Use training=True since we have dropout (acts differently in tng vs inference,\n",
        "        # see here: https://www.tensorflow.org/tutorials/quickstart/advanced)\n",
        "        y_hat = model(x, training=True)\n",
        "\n",
        "        # get the input label\n",
        "        y_sample = tf.math.argmax(y, axis=1, output_type=tf.dtypes.int64)\n",
        "        # tf.print(y, summarize=-1)\n",
        "        # tf.print(y_sample, summarize=-1)\n",
        "\n",
        "        loss = compute_loss(y_sample, y_hat[:, -1, :])\n",
        "\n",
        "        # Compute the gradients\n",
        "        # We want the gradient of the loss with respect to all of the model parameters.\n",
        "        # Use `model.trainable_variables` to get a list of all model parameters.\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Apply the gradients to the optimizer so it can update the model accordingly\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "##################\n",
        "# Begin training!#\n",
        "##################\n",
        "\n",
        "# history = []\n",
        "# # plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
        "# plt.title('Loss over time')\n",
        "# plt.xlabel('Iterations')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "# if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "# for iter in tqdm(range(num_training_iterations)):\n",
        "\n",
        "#   # Grab a batch and propagate it through the network\n",
        "#   x_batch, y_batch = get_batch(vector_chats.numpy(), vector_target, batch_size=batch_size)\n",
        "#   loss = train_step(x_batch, y_batch)\n",
        "\n",
        "# #   print(\"Input shape:      \", x_batch.shape, \" # (batch_size, sequence_length)\")\n",
        "# #   print(\"Prediction shape: \", y_batch.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "#   # Update the progress bar\n",
        "#   history.append(loss.numpy().mean())\n",
        "\n",
        "#   # Update the model with the changed weights!\n",
        "#   if iter % 100 == 0:\n",
        "#     model.save_weights(checkpoint_prefix)\n",
        "#     plt.plot(range(iter + 1), history, 'g', label='Training loss')\n",
        "#     plt.show()\n",
        "\n",
        "# # Save the trained model and the weights\n",
        "# model.save_weights(checkpoint_prefix)\n",
        "# # plt.plot(epochs, loss_val, 'b', label='validation loss')    plt.show()\n",
        "# plt.plot(range(101,num_training_iterations), history[101:num_training_iterations], 'g', label='Training loss')\n",
        "\n",
        "\n",
        "############## Inference ######################\n",
        "\n",
        "\n",
        "# batch size None for inference, remove statefulness and allow any size input\n",
        "model = build_model(vocab_size=len(vocab), num_class=num_classes, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=None)\n",
        "\n",
        "# Restore the model weights for the last checkpoint after training\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "## Prediction of a chat class ###\n",
        "def classify(chats):\n",
        "\n",
        "    # convert to list if needed\n",
        "    chats = [chats] if isinstance(chats, str) else chats\n",
        "\n",
        "    # up the dimension if single inference...its hacky i know...\n",
        "    single_item = False\n",
        "    if len(chats) == 1:\n",
        "       chats.append(\"\")\n",
        "       single_item = True\n",
        "\n",
        "    # Evaluation step (generating ABC text using the learned RNN model)\n",
        "    input_eval = vectorize_layer(tf.squeeze(chats))\n",
        "    pred = model(input_eval)\n",
        "    pred = tf.nn.softmax(tf.squeeze(pred)[:, -1, :])\n",
        "    output_labels = tf.argmax(pred, axis=1)\n",
        "\n",
        "    return output_labels if not single_item else [output_labels[0]]\n",
        "\n",
        "\n",
        "def classify_label(chats):\n",
        "    encoded_labels = classify(chats)\n",
        "    labels = ['recycle', 'review', 'action']\n",
        "    return list(map(lambda label: labels[label], encoded_labels))\n",
        "\n",
        "# calculate accuracy\n",
        "pred_labels = classify(train_text)\n",
        "accuracy = np.sum(np.equal(\n",
        "    list(map(np.argmax, vector_target)), pred_labels))/len(train_text)\n",
        "print(f'Accuracy is {accuracy}')"
      ]
    }
  ]
}